{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bfe1bc9ebb14bcb9d2eefba702cbe26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c9c733bfad04f4da026deef12e33366",
              "IPY_MODEL_ab3d151d0bb44cf68fde93376822ba0f",
              "IPY_MODEL_deb1d209ef0f4b01890a49ed753df07e"
            ],
            "layout": "IPY_MODEL_890d245c8460490c9866b66c42a58310"
          }
        },
        "0c9c733bfad04f4da026deef12e33366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac41fb08a2f5445e8e8344a3bb140489",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83ad9474212344c2b340df82b2f2f92d",
            "value": "Cropping‚Äáimages:‚Äá100%"
          }
        },
        "ab3d151d0bb44cf68fde93376822ba0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa1c1530e5044fa381930863c8d8e7b4",
            "max": 30051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_158406f2268c45f59d352e22c6af58f3",
            "value": 30051
          }
        },
        "deb1d209ef0f4b01890a49ed753df07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88a5e868a140445cbf144ae32fbb4eea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0213969f890940cba8f76efe22819720",
            "value": "‚Äá30051/30051‚Äá[04:04&lt;00:00,‚Äá129.17it/s]"
          }
        },
        "890d245c8460490c9866b66c42a58310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac41fb08a2f5445e8e8344a3bb140489": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ad9474212344c2b340df82b2f2f92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa1c1530e5044fa381930863c8d8e7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "158406f2268c45f59d352e22c6af58f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88a5e868a140445cbf144ae32fbb4eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0213969f890940cba8f76efe22819720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OkM7lfC1YsI",
        "outputId": "9c7aff69-4308-43f2-c902-f8a8d99b4fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setup Shuru ---\n",
            "Target Folder: /content/local_images\n",
            "Copying Copy of images_archive.tar.gz...\n",
            "Copy complete in 34.51 seconds.\n",
            "Extracting and ADDING images to /content/local_images...\n",
            "Extraction complete in 30.29 seconds.\n",
            "\n",
            "‚úÖ --- READY TO TRAIN! Images are in /content/local_images ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# --- 1. Sirf is line ko change karein ---\n",
        "# üî¥ Pehli baar 'images1_archive.tar.gz' likhein\n",
        "#    Doosri baar 'images2_archive.tar.gz' likhein, etc.\n",
        "ARCHIVE_FILE_NAME = \"Copy of images_archive.tar.gz\"\n",
        "\n",
        "# --- 2. In paths ko hamesha same rehne dein ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/images\"\n",
        "# Yeh hamesha same folder rahega taake images jama (collect) ho sakein\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "# --- 3. Baqi code ab automatically kaam karega ---\n",
        "DRIVE_TAR_PATH = os.path.join(DRIVE_BASE_PATH, ARCHIVE_FILE_NAME)\n",
        "LOCAL_TAR_PATH = f\"/content/{ARCHIVE_FILE_NAME}\"\n",
        "\n",
        "# Yeh line check karti hai ke folder hai ya nahi. Agar hai, to usay istemaal karti hai.\n",
        "os.makedirs(LOCAL_IMAGE_DIR, exist_ok=True)\n",
        "print(f\"--- Setup Shuru ---\")\n",
        "print(f\"Target Folder: {LOCAL_IMAGE_DIR}\")\n",
        "\n",
        "# --- FAST COPY ---\n",
        "print(f\"Copying {ARCHIVE_FILE_NAME}...\")\n",
        "start_time = time.time()\n",
        "!cp \"{DRIVE_TAR_PATH}\" \"{LOCAL_TAR_PATH}\"\n",
        "print(f\"Copy complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "# --- FAST UNPACK (ADD) ---\n",
        "print(f\"Extracting and ADDING images to {LOCAL_IMAGE_DIR}...\")\n",
        "start_time = time.time()\n",
        "# Yeh command purani files ko delete nahi karta, sirf nayi files add karta hai\n",
        "!tar -xzf \"{LOCAL_TAR_PATH}\" -C \"{LOCAL_IMAGE_DIR}\"\n",
        "print(f\"Extraction complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "print(f\"\\n‚úÖ --- READY TO TRAIN! Images are in {LOCAL_IMAGE_DIR} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The path where you extracted the images\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "try:\n",
        "    file_count = len(os.listdir(LOCAL_IMAGE_DIR))\n",
        "    print(f\"‚úÖ Success! Found {file_count} images in '{LOCAL_IMAGE_DIR}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Could not find the directory '{LOCAL_IMAGE_DIR}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ucXuV-92b8Z",
        "outputId": "fda7a123-9c69-4a1d-a17c-5a0a970d9fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success! Found 30051 images in '/content/local_images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2  # OpenCV (Colab mein pehle se install hota hai)\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np # Numpy zaroori hai\n",
        "\n",
        "# --- 1. Paths ---\n",
        "SOURCE_DIR = \"/content/local_images\"\n",
        "DEST_DIR = \"/content/local_images_cropped\"\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "# --- 2. Crop Setting ---\n",
        "# Hum 75% istemaal karein ge taake \"L\" aur \"PORTABLE\" text zaroor cut jaayein\n",
        "CROP_PERCENT = 0.75\n",
        "\n",
        "print(f\"'{SOURCE_DIR}' se images ko crop kiya ja raha hai...\")\n",
        "print(f\"Nayi images '{DEST_DIR}' mein save hon gi...\")\n",
        "print(f\"Cropping percentage: {int(CROP_PERCENT*100)}%\")\n",
        "\n",
        "image_files = os.listdir(SOURCE_DIR)\n",
        "errors = 0\n",
        "processed_count = 0\n",
        "\n",
        "for filename in tqdm(image_files, desc=\"Cropping images\"):\n",
        "    source_path = os.path.join(SOURCE_DIR, filename)\n",
        "    dest_path = os.path.join(DEST_DIR, filename)\n",
        "\n",
        "    try:\n",
        "        # Image ko OpenCV se load karein\n",
        "        img = cv2.imread(source_path)\n",
        "        if img is None:\n",
        "            # print(f\"Warning: {filename} load nahi ho saki, skip kar raha hoon.\")\n",
        "            errors += 1\n",
        "            continue\n",
        "\n",
        "        # Image ki height aur width lein\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # --- Center Crop Logic ---\n",
        "        new_h = int(h * CROP_PERCENT)\n",
        "        new_w = int(w * CROP_PERCENT)\n",
        "\n",
        "        start_y = (h - new_h) // 2\n",
        "        start_x = (w - new_w) // 2\n",
        "        end_y = start_y + new_h\n",
        "        end_x = start_x + new_w\n",
        "\n",
        "        # Image ko crop karein\n",
        "        cropped_img = img[start_y:end_y, start_x:end_x]\n",
        "\n",
        "        # Nayi image ko save karein\n",
        "        cv2.imwrite(dest_path, cropped_img)\n",
        "        processed_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error file {filename}: {e}\")\n",
        "        errors += 1\n",
        "\n",
        "print(\"\\n--- CROP MUKAMMAL! ---\")\n",
        "print(f\"Total images found: {len(image_files)}\")\n",
        "print(f\"Successfully processed and saved: {processed_count}\")\n",
        "print(f\"Errors (skipped files): {errors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "2bfe1bc9ebb14bcb9d2eefba702cbe26",
            "0c9c733bfad04f4da026deef12e33366",
            "ab3d151d0bb44cf68fde93376822ba0f",
            "deb1d209ef0f4b01890a49ed753df07e",
            "890d245c8460490c9866b66c42a58310",
            "ac41fb08a2f5445e8e8344a3bb140489",
            "83ad9474212344c2b340df82b2f2f92d",
            "fa1c1530e5044fa381930863c8d8e7b4",
            "158406f2268c45f59d352e22c6af58f3",
            "88a5e868a140445cbf144ae32fbb4eea",
            "0213969f890940cba8f76efe22819720"
          ]
        },
        "id": "O3tQ12Em2jIe",
        "outputId": "5e1ace6c-c844-42b1-dac1-f45c540f52ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/local_images' se images ko crop kiya ja raha hai...\n",
            "Nayi images '/content/local_images_cropped' mein save hon gi...\n",
            "Cropping percentage: 75%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cropping images:   0%|          | 0/30051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bfe1bc9ebb14bcb9d2eefba702cbe26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CROP MUKAMMAL! ---\n",
            "Total images found: 30051\n",
            "Successfully processed and saved: 30051\n",
            "Errors (skipped files): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandAugment\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. Configuration (NAYI SETTINGS) ---\n",
        "BATCH_SIZE = 64 # üî¥ 64 par waapas (taake tez ho)\n",
        "IMG_WIDTH, IMG_HEIGHT = 256, 256 # üî¥ 300x300 se 256x256 karein (Speed ke liye)\n",
        "EPOCHS_FINE_TUNE = 30\n",
        "LEARNING_RATE_FINE_TUNE = 1e-5 # üî¥ Bohat Low LR (RandAugment ke liye)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "# üî¥ Yaqeen karein ke yeh aapka 86.35% wala model hai\n",
        "SAVED_WEIGHTS_PATH = '/content/drive/MyDrive/images/v5_randaugment_best(Final1).keras'\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/images/v6_okay_final_256px.keras' # Naya naam\n",
        "IMAGE_DIR = '/content/local_images_cropped' # Cropped folder\n",
        "CLEAN_CSV_PATH = 'master_tf_clean.csv'\n",
        "\n",
        "# --- 3. Class Weights and Names ---\n",
        "# üî¥ Naye Class Weights (Viral = 4.0)\n",
        "CLASS_WEIGHTS_DICT = {\n",
        "    0: 1.38, 1: 0.83, 2: 0.33, 3: 2.23, 4: 4.0, 5: 7.15\n",
        "}\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 4. Function Definitions ---\n",
        "\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) # üî¥ Naya Size (256)\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "# RandAugment wala function\n",
        "def build_dataset(df, augment=False):\n",
        "    augmentation_layers = tf.keras.Sequential([\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(0.1),\n",
        "        RandomZoom(0.1),\n",
        "        RandAugment(value_range=(0, 255), num_ops=2, factor=0.2)\n",
        "    ], name=\"augmentation\")\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (augmentation_layers(x, training=True), y),\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(BATCH_SIZE) # üî¥ Naya Batch Size (64)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Model build function (Crash Fix ke saath)\n",
        "def build_densenet_model():\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)) # üî¥ Naya Size (256)\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model, base_model\n",
        "\n",
        "# --- 5. Data Loading & Pipeline Execution ---\n",
        "print(f\"Loading data from {CLEAN_CSV_PATH}...\")\n",
        "df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "df['label_idx'] = df['label'].map(class_indices)\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "# üî¥ Hum sirf Stage 2 (RandAugment) dataset banayein ge\n",
        "train_dataset = build_dataset(train_df, augment=True)\n",
        "val_dataset = build_dataset(val_df, augment=False)\n",
        "test_dataset = build_dataset(test_df, augment=False)\n",
        "\n",
        "# --- 6. Model Build and Weight Load (Skip Stage 1) ---\n",
        "print(\"Building DenseNet-121 model structure (256x256)...\")\n",
        "model, base_model = build_densenet_model()\n",
        "\n",
        "print(f\"--- STAGE 1 SKIPPED ---\")\n",
        "print(f\"Loading best weights (86.35%) from '{SAVED_WEIGHTS_PATH}'...\")\n",
        "try:\n",
        "    model.load_weights(SAVED_WEIGHTS_PATH)\n",
        "    print(\"‚úÖ Weights loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Weights load nahi huin. {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 7. STAGE 2: Smart Fine-Tuning (RandAugment ke saath) ---\n",
        "print(\"\\n--- STAGE 2: Smart Fine-tuning (RandAugment Enabled) ---\")\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:347]: # Top layers ko fine-tune karein\n",
        "    layer.trainable = False\n",
        "print(\"‚úÖ DenseNet Base Model ki Top Layers Fine-tuning ke liye unfreeze kar di gayi hain.\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE), # üî¥ Bohat Low LR\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stopping_s2 = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint_s2 = ModelCheckpoint(FINAL_MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, verbose=1)\n",
        "\n",
        "print(\"Starting fine-tuning with RandAugment (256x256)...\")\n",
        "\n",
        "history_fine_tune = model.fit(\n",
        "    train_dataset, # üî¥ RandAugment dataset\n",
        "    epochs=EPOCHS_FINE_TUNE,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping_s2, model_checkpoint_s2, reduce_lr],\n",
        "    class_weight=CLASS_WEIGHTS_DICT\n",
        ")\n",
        "\n",
        "print(\"--- RandAugment Fine-Tuning Complete! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkKrvDR820dK",
        "outputId": "6c065ed5-e65a-4147-f7ff-aeb3143e8085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from master_tf_clean.csv...\n",
            "Building DenseNet-121 model structure (256x256)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "--- STAGE 1 SKIPPED ---\n",
            "Loading best weights (86.35%) from '/content/drive/MyDrive/images/v5_randaugment_best(Final1).keras'...\n",
            "‚úÖ Weights loaded successfully.\n",
            "\n",
            "--- STAGE 2: Smart Fine-tuning (RandAugment Enabled) ---\n",
            "‚úÖ DenseNet Base Model ki Top Layers Fine-tuning ke liye unfreeze kar di gayi hain.\n",
            "Starting fine-tuning with RandAugment (256x256)...\n",
            "Epoch 1/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6589 - loss: 0.9030\n",
            "Epoch 1: val_loss improved from inf to 0.65687, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 2s/step - accuracy: 0.6589 - loss: 0.9029 - val_accuracy: 0.7703 - val_loss: 0.6569 - learning_rate: 1.0000e-05\n",
            "Epoch 2/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6880 - loss: 0.8257\n",
            "Epoch 2: val_loss improved from 0.65687 to 0.64960, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 2s/step - accuracy: 0.6880 - loss: 0.8257 - val_accuracy: 0.7746 - val_loss: 0.6496 - learning_rate: 1.0000e-05\n",
            "Epoch 3/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7013 - loss: 0.7955\n",
            "Epoch 3: val_loss improved from 0.64960 to 0.63854, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 2s/step - accuracy: 0.7013 - loss: 0.7954 - val_accuracy: 0.7783 - val_loss: 0.6385 - learning_rate: 1.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7087 - loss: 0.7526\n",
            "Epoch 4: val_loss improved from 0.63854 to 0.63762, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2s/step - accuracy: 0.7087 - loss: 0.7526 - val_accuracy: 0.7743 - val_loss: 0.6376 - learning_rate: 1.0000e-05\n",
            "Epoch 5/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7210 - loss: 0.7258\n",
            "Epoch 5: val_loss improved from 0.63762 to 0.61666, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 2s/step - accuracy: 0.7210 - loss: 0.7258 - val_accuracy: 0.7843 - val_loss: 0.6167 - learning_rate: 1.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7313 - loss: 0.7084\n",
            "Epoch 6: val_loss did not improve from 0.61666\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 2s/step - accuracy: 0.7313 - loss: 0.7084 - val_accuracy: 0.7796 - val_loss: 0.6210 - learning_rate: 1.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7340 - loss: 0.7026\n",
            "Epoch 7: val_loss improved from 0.61666 to 0.60672, saving model to /content/drive/MyDrive/images/v6_okay_final_256px.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 2s/step - accuracy: 0.7340 - loss: 0.7026 - val_accuracy: 0.7833 - val_loss: 0.6067 - learning_rate: 1.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7444 - loss: 0.6861\n",
            "Epoch 8: val_loss did not improve from 0.60672\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 2s/step - accuracy: 0.7444 - loss: 0.6860 - val_accuracy: 0.7803 - val_loss: 0.6249 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7479 - loss: 0.6714\n",
            "Epoch 9: val_loss did not improve from 0.60672\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 2s/step - accuracy: 0.7479 - loss: 0.6714 - val_accuracy: 0.7746 - val_loss: 0.6357 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7541 - loss: 0.6629\n",
            "Epoch 10: val_loss did not improve from 0.60672\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 2s/step - accuracy: 0.7541 - loss: 0.6629 - val_accuracy: 0.7726 - val_loss: 0.6428 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7585 - loss: 0.6414\n",
            "Epoch 11: val_loss did not improve from 0.60672\n",
            "\u001b[1m376/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 2s/step - accuracy: 0.7585 - loss: 0.6414 - val_accuracy: 0.7683 - val_loss: 0.6680 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m281/376\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m2:46\u001b[0m 2s/step - accuracy: 0.7618 - loss: 0.6420"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FstZrdmW5MJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}