{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "FNUeaXOBEdC_",
        "outputId": "4e6a3e80-d91b-46da-f92a-96c9f0aa706a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing dataset...\n",
            "Found 5071 files belonging to 2 classes.\n",
            "Using 4057 files for training.\n",
            "Using 1014 files for validation.\n",
            "Classes found: ['chest_xray', 'not_xray']\n",
            "Building the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16384\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m2,097,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16384</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,120,993\u001b[0m (8.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,120,993</span> (8.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,120,993\u001b[0m (8.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,120,993</span> (8.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "DATA_DIR = '/content/drive/MyDrive/Model /dataset'  # The folder you created in Step 1\n",
        "\n",
        "# --- 2. Load the Data ---\n",
        "print(\"Loading and preparing dataset...\")\n",
        "\n",
        "# Load images from the directory\n",
        "# 'label_mode' is 'binary' because we have only two classes.\n",
        "# 'validation_split' reserves 20% of the data for testing the model's accuracy.\n",
        "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"both\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "# Get the class names (will be ['chest_xray', 'not_xray'])\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Classes found: {class_names}\")\n",
        "\n",
        "# Configure dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# --- 3. Build the CNN Model ---\n",
        "print(\"Building the model...\")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # Rescale pixel values from [0, 255] to [0, 1]\n",
        "    layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Third convolutional block\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flatten the 3D features into a 1D vector\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # A dense (fully-connected) layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "\n",
        "    # Output layer: 1 neuron with a 'sigmoid' activation.\n",
        "    # Sigmoid is perfect for binary classification (outputs a value between 0 and 1)\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# --- 4. Compile the Model ---\n",
        "# We use 'binary_crossentropy' for a 2-class problem with a sigmoid output.\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Show a summary of the model\n",
        "model.summary()\n",
        "\n",
        "# --- 5. Train the Model ---\n",
        "print(\"Training the model...\")\n",
        "EPOCHS = 10  # You can increase this if accuracy is low\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# --- 6. Save the Model ---\n",
        "MODEL_NAME = 'chest_xray_detector.keras'\n",
        "model.save(MODEL_NAME)\n",
        "print(f\"\\nModel training complete! Saved as {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rx07GIasE6o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import sys\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "MODEL_NAME = 'chest_xray_detector.keras'\n",
        "\n",
        "# --- 2. Load the Model ---\n",
        "try:\n",
        "    model = tf.keras.models.load_model(MODEL_NAME)\n",
        "    print(f\"Model '{MODEL_NAME}' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Please make sure you have run 'train.py' first to create the model file.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- 3. Create Prediction Function ---\n",
        "def classify_image(image_path):\n",
        "    \"\"\"\n",
        "    Loads an image, preprocesses it, and predicts if it's a chest X-ray.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the image and resize it to the model's expected input size\n",
        "        img = load_img(\n",
        "            image_path,\n",
        "            target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        "        )\n",
        "\n",
        "        # Convert the image to a NumPy array\n",
        "        img_array = img_to_array(img)\n",
        "\n",
        "        # Add an extra dimension to create a \"batch\" of 1\n",
        "        # The model expects input shape (batch_size, height, width, channels)\n",
        "        img_batch = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Note: We don't need to rescale by 1./255 here because\n",
        "        # the 'Rescaling' layer is already part of the saved model.\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(img_batch, verbose=0)\n",
        "\n",
        "        # The output is a value between 0 and 1 from the sigmoid function\n",
        "        score = prediction[0][0]\n",
        "\n",
        "        # We assume 'chest_xray' is class 0 and 'not_xray' is class 1\n",
        "        # (This depends on the order tf.keras.utils.image_dataset_from_directory finds them)\n",
        "        # Let's check the score. A score < 0.5 means class 0 (chest_xray)\n",
        "\n",
        "        if score < 0.5:\n",
        "            # Note: We use (1 - score) because accuracy is usually presented\n",
        "            # as confidence in the *predicted* class.\n",
        "            print(f\"This is a: CHEST X-RAY (Confidence: {100 * (1 - score):.2f}%)\")\n",
        "        else:\n",
        "            print(f\"This is a: NOT A CHEST X-RAY (Confidence: {100 * score:.2f}%)\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{image_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# --- 4. Use the Function ---\n",
        "# Get the image path from the command line argument\n",
        "if len(sys.argv) < 2:\n",
        "    print(\"How to use: python predict.py <path_to_your_image.jpg>\")\n",
        "    # Example:\n",
        "    # python predict.py my_xray_test.jpg\n",
        "    # python predict.py my_cat_photo.jpg\n",
        "else:\n",
        "    image_to_test = \"/content/drive/MyDrive/Model /dataset/chest_xray/person88_virus_167.jpeg\"\n",
        "    classify_image(image_to_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb4S75ZYFIHR",
        "outputId": "8f5f7054-aaab-42f4-c179-a556ca758775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'chest_xray_detector.keras' loaded successfully.\n",
            "This is a: CHEST X-RAY (Confidence: 99.99%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tn5zvXvMFIEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoZYPrJk5xm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YA9atRDw5xl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfD8D2iW5xiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "print(\"--- Step 1: Mounting Google Drive ---\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# --- 2. Define Paths ---\n",
        "# SLOW path on your Google Drive\n",
        "DRIVE_SOURCE_DIR = '/content/drive/MyDrive/Model /dataset'\n",
        "\n",
        "# FAST path on the local Colab disk\n",
        "LOCAL_DATA_DIR = '/content/dataset'\n",
        "\n",
        "# Where to save the final, trained model\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/Model /chest_xray_detector_mobilenet.keras'\n",
        "\n",
        "\n",
        "# --- 3. Copy Data from Drive to Local Disk (The Speed Fix) ---\n",
        "print(f\"\\n--- Step 2: Copying dataset from {DRIVE_SOURCE_DIR} ---\")\n",
        "print(\"This may take a few minutes, but you only do it once.\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create the local destination folder\n",
        "os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Use the 'cp' command to copy. It's much faster than Python's 'shutil'\n",
        "# -r = recursive (all subfolders)\n",
        "# -n = no-clobber (don't re-copy files that already exist)\n",
        "!cp -r -n \"{DRIVE_SOURCE_DIR}\"/* \"{LOCAL_DATA_DIR}\"/\n",
        "\n",
        "print(f\"Copying complete! Took {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# --- 4. Configure GPU ---\n",
        "print(\"\\n--- Step 3: Configuring GPU ---\")\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"Found {len(gpus)} GPUs.\")\n",
        "    try:\n",
        "        # Enable 'memory growth'\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"Memory growth enabled for GPUs.\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"WARNING: No GPU found. Training will use the CPU.\")\n",
        "\n",
        "\n",
        "# --- 5. Set Model Configuration ---\n",
        "print(\"\\n--- Step 4: Setting Configuration ---\")\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 256  # <-- INCREASED BATCH SIZE to feed the GPU\n",
        "EPOCHS = 10\n",
        "print(f\"Batch size set to: {BATCH_SIZE}\")\n",
        "\n",
        "\n",
        "# --- 6. Load Data (from the FAST local path) ---\n",
        "print(f\"\\n--- Step 5: Loading data from {LOCAL_DATA_DIR} ---\")\n",
        "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    LOCAL_DATA_DIR,  # <-- USING FAST LOCAL PATH\n",
        "    validation_split=0.2,\n",
        "    subset=\"both\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Classes found: {class_names}\")\n",
        "\n",
        "\n",
        "# --- 7. Preprocess Data ---\n",
        "print(\"\\n--- Step 6: Preprocessing data for MobileNetV2 ---\")\n",
        "def preprocess_data(image, label):\n",
        "    return preprocess_input(image), label\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE).cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_data, num_parallel_calls=AUTOTUNE).cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(\"Dataset preprocessing and caching complete.\")\n",
        "\n",
        "\n",
        "# --- 8. Build the Model with Transfer Learning ---\n",
        "print(\"\\n--- Step 7: Building MobileNetV2 model ---\")\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(IMG_WIDTH, IMG_WIDTH, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add our custom head\n",
        "model = keras.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# --- 9. Compile the Model ---\n",
        "print(\"\\n--- Step 8: Compiling model ---\")\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# --- 10. Train the Model ---\n",
        "print(\"\\n--- Step 9: Starting model training (This should be fast now) ---\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "\n",
        "# --- 11. Save the Final Model to Google Drive ---\n",
        "print(f\"\\n--- Step 10: Saving trained model to {MODEL_SAVE_PATH} ---\")\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"Model training complete! Saved to {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IBsV7ZxV6eW2",
        "outputId": "2966ef87-5ecd-403f-fbc3-5492c982ad06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 1: Mounting Google Drive ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "--- Step 2: Copying dataset from /content/drive/MyDrive/Model /dataset ---\n",
            "This may take a few minutes, but you only do it once.\n",
            "Copying complete! Took 1.11 seconds.\n",
            "\n",
            "--- Step 3: Configuring GPU ---\n",
            "Found 1 GPUs.\n",
            "Physical devices cannot be modified after being initialized\n",
            "\n",
            "--- Step 4: Setting Configuration ---\n",
            "Batch size set to: 256\n",
            "\n",
            "--- Step 5: Loading data from /content/dataset ---\n",
            "Found 5071 files belonging to 2 classes.\n",
            "Using 4057 files for training.\n",
            "Using 1014 files for validation.\n",
            "Classes found: ['chest_xray', 'not_xray']\n",
            "\n",
            "--- Step 6: Preprocessing data for MobileNetV2 ---\n",
            "Dataset preprocessing and caching complete.\n",
            "\n",
            "--- Step 7: Building MobileNetV2 model ---\n",
            "\n",
            "--- Step 8: Compiling model ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ mobilenetv2_1.00_128            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,259,265\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,259,265</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,281\u001b[0m (5.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> (5.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 9: Starting model training (This should be fast now) ---\n",
            "Epoch 1/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 963ms/step - accuracy: 0.8967 - loss: 0.2575 - val_accuracy: 0.9744 - val_loss: 0.1114\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9778 - loss: 0.0815 - val_accuracy: 0.9842 - val_loss: 0.0339\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9896 - loss: 0.0271 - val_accuracy: 0.9980 - val_loss: 0.0123\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9994 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0083\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0047\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "\n",
            "--- Step 10: Saving trained model to /content/drive/MyDrive/Model /chest_xray_detector_mobilenet.keras ---\n",
            "Model training complete! Saved to /content/drive/MyDrive/Model /chest_xray_detector_mobilenet.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2GKBGxt8bDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# --- 1. Mount Drive and Load Model ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_PATH = '/content/drive/MyDrive/Model/chest_xray_detector_mobilenet.keras'\n",
        "IMAGE_TO_TEST = '/content/dataset/not_xray/some_test_image.jpg' # Change this path\n",
        "\n",
        "# Load the model\n",
        "try:\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    print(f\"Model loaded successfully from {MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Make sure training is complete and the path is correct.\")\n",
        "\n",
        "# --- 2. Prediction Function ---\n",
        "def classify_image(image_path, model):\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Error: Image not found at {image_path}\")\n",
        "        # Try checking your Google Drive path if you didn't copy\n",
        "        drive_image_path = f\"/content/drive/MyDrive/Model /dataset/not_xray/{os.path.basename(image_path)}\"\n",
        "        if os.path.exists(drive_image_path):\n",
        "            print(f\"Found it in Drive, using: {drive_image_path}\")\n",
        "            image_path = drive_image_path\n",
        "        else:\n",
        "            print(f\"Also not found in Drive: {drive_image_path}\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        img = load_img(image_path, target_size=(128, 128))\n",
        "        img_array = img_to_array(img)\n",
        "        img_batch = np.expand_dims(img_array, axis=0)\n",
        "        img_preprocessed = preprocess_input(img_batch)\n",
        "\n",
        "        prediction = model.predict(img_preprocessed, verbose=0)\n",
        "        score = prediction[0][0]\n",
        "\n",
        "        if score < 0.5:\n",
        "            confidence = 100 * (1 - score)\n",
        "            print(f\"Prediction: This is a CHEST X-RAY (Confidence: {confidence:.2f}%)\")\n",
        "        else:\n",
        "            confidence = 100 * score\n",
        "            print(f\"Prediction: This is NOT A CHEST X-RAY (Confidence: {confidence:.2f}%)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "\n",
        "# --- 3. Test the function ---\n",
        "# Make sure to change IMAGE_TO_TEST to a real image path\n",
        "# You can upload one to Colab or use a path from your copied dataset\n",
        "classify_image(IMAGE_TO_TEST, model)"
      ],
      "metadata": {
        "id": "__5WqXiu-IpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yoi0SYL-IlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwyX8ZHa-Iju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}