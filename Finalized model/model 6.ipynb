{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bb2ffb38ddd4fe0b1eee2e2f0231fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3628d09ca07049beaafe1c0bfb1087d5",
              "IPY_MODEL_19732df10eb8441aafc65e9cce78e62c",
              "IPY_MODEL_26e221de0533475aa4ebc6bbc5466a31"
            ],
            "layout": "IPY_MODEL_1610d70b061d4c71a6bd249cb1d219c9"
          }
        },
        "3628d09ca07049beaafe1c0bfb1087d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991f34b052f44c4db54924c93298a17c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a6a9c37ffe2047f7b42da96cdf847810",
            "value": "Cropping‚Äáimages:‚Äá100%"
          }
        },
        "19732df10eb8441aafc65e9cce78e62c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d81685027740b28e574cdc70c228c7",
            "max": 30051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_052d353a3d794a5f828c4779edfb2037",
            "value": 30051
          }
        },
        "26e221de0533475aa4ebc6bbc5466a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93264d9dd054bf2b8bfaab531d52889",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_44bc0623853648a0b267df137804e59f",
            "value": "‚Äá30051/30051‚Äá[03:51&lt;00:00,‚Äá103.78it/s]"
          }
        },
        "1610d70b061d4c71a6bd249cb1d219c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991f34b052f44c4db54924c93298a17c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a9c37ffe2047f7b42da96cdf847810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00d81685027740b28e574cdc70c228c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "052d353a3d794a5f828c4779edfb2037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a93264d9dd054bf2b8bfaab531d52889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bc0623853648a0b267df137804e59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61ccf844868e4fa5ba71ae7369eaac15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f833527c228f4372b136c01fc7e51649",
              "IPY_MODEL_9797662ea0e94dd48cb9a1d5b2751aae",
              "IPY_MODEL_60fb81141abe411aa0f3ca1b32b1940c"
            ],
            "layout": "IPY_MODEL_0a0c7723049e4dbabf8777f46f64201b"
          }
        },
        "f833527c228f4372b136c01fc7e51649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c3bfe514199481ea604a77f3c766f91",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c09875eadd8b4f82a3f572b1da704390",
            "value": "Generating‚ÄáPredictions:‚Äá100%"
          }
        },
        "9797662ea0e94dd48cb9a1d5b2751aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f74a4e24e058460a9983715badc90156",
            "max": 94,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_962a30ef824f410bb1ebb3832d91c53b",
            "value": 94
          }
        },
        "60fb81141abe411aa0f3ca1b32b1940c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52220efb599b4f22b1a7be12a79c308e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a197927e8624f8f8a455f2f7795b349",
            "value": "‚Äá94/94‚Äá[01:15&lt;00:00,‚Äá‚Äá7.03s/it]"
          }
        },
        "0a0c7723049e4dbabf8777f46f64201b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3bfe514199481ea604a77f3c766f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09875eadd8b4f82a3f572b1da704390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f74a4e24e058460a9983715badc90156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962a30ef824f410bb1ebb3832d91c53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52220efb599b4f22b1a7be12a79c308e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a197927e8624f8f8a455f2f7795b349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN7Kv3mK5MJ5",
        "outputId": "fa0b6daf-c3be-48ba-b21a-fda9fadca08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setup Shuru ---\n",
            "Target Folder: /content/local_images\n",
            "Copying Copy of images2_archive.tar.gz...\n",
            "Copy complete in 19.21 seconds.\n",
            "Extracting and ADDING images to /content/local_images...\n",
            "Extraction complete in 6.34 seconds.\n",
            "\n",
            "‚úÖ --- READY TO TRAIN! Images are in /content/local_images ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# --- 1. Sirf is line ko change karein ---\n",
        "# üî¥ Pehli baar 'images1_archive.tar.gz' likhein\n",
        "#    Doosri baar 'images2_archive.tar.gz' likhein, etc.\n",
        "ARCHIVE_FILE_NAME = \"Copy of images2_archive.tar.gz\"\n",
        "\n",
        "# --- 2. In paths ko hamesha same rehne dein ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/images\"\n",
        "# Yeh hamesha same folder rahega taake images jama (collect) ho sakein\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "# --- 3. Baqi code ab automatically kaam karega ---\n",
        "DRIVE_TAR_PATH = os.path.join(DRIVE_BASE_PATH, ARCHIVE_FILE_NAME)\n",
        "LOCAL_TAR_PATH = f\"/content/{ARCHIVE_FILE_NAME}\"\n",
        "\n",
        "# Yeh line check karti hai ke folder hai ya nahi. Agar hai, to usay istemaal karti hai.\n",
        "os.makedirs(LOCAL_IMAGE_DIR, exist_ok=True)\n",
        "print(f\"--- Setup Shuru ---\")\n",
        "print(f\"Target Folder: {LOCAL_IMAGE_DIR}\")\n",
        "\n",
        "# --- FAST COPY ---\n",
        "print(f\"Copying {ARCHIVE_FILE_NAME}...\")\n",
        "start_time = time.time()\n",
        "!cp \"{DRIVE_TAR_PATH}\" \"{LOCAL_TAR_PATH}\"\n",
        "print(f\"Copy complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "# --- FAST UNPACK (ADD) ---\n",
        "print(f\"Extracting and ADDING images to {LOCAL_IMAGE_DIR}...\")\n",
        "start_time = time.time()\n",
        "# Yeh command purani files ko delete nahi karta, sirf nayi files add karta hai\n",
        "!tar -xzf \"{LOCAL_TAR_PATH}\" -C \"{LOCAL_IMAGE_DIR}\"\n",
        "print(f\"Extraction complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "print(f\"\\n‚úÖ --- READY TO TRAIN! Images are in {LOCAL_IMAGE_DIR} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The path where you extracted the images\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "try:\n",
        "    file_count = len(os.listdir(LOCAL_IMAGE_DIR))\n",
        "    print(f\"‚úÖ Success! Found {file_count} images in '{LOCAL_IMAGE_DIR}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Could not find the directory '{LOCAL_IMAGE_DIR}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aEvzHP256gQ",
        "outputId": "bca3568e-dd51-4017-d5dc-9f310d3977ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success! Found 30051 images in '/content/local_images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2  # OpenCV (Colab mein pehle se install hota hai)\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np # Numpy zaroori hai\n",
        "\n",
        "# --- 1. Paths ---\n",
        "SOURCE_DIR = \"/content/local_images\"\n",
        "DEST_DIR = \"/content/local_images_cropped\"\n",
        "os.makedirs(DEST_DIR, exist_ok=True)\n",
        "\n",
        "# --- 2. Crop Setting ---\n",
        "# Hum 75% istemaal karein ge taake \"L\" aur \"PORTABLE\" text zaroor cut jaayein\n",
        "CROP_PERCENT = 0.75\n",
        "\n",
        "print(f\"'{SOURCE_DIR}' se images ko crop kiya ja raha hai...\")\n",
        "print(f\"Nayi images '{DEST_DIR}' mein save hon gi...\")\n",
        "print(f\"Cropping percentage: {int(CROP_PERCENT*100)}%\")\n",
        "\n",
        "image_files = os.listdir(SOURCE_DIR)\n",
        "errors = 0\n",
        "processed_count = 0\n",
        "\n",
        "for filename in tqdm(image_files, desc=\"Cropping images\"):\n",
        "    source_path = os.path.join(SOURCE_DIR, filename)\n",
        "    dest_path = os.path.join(DEST_DIR, filename)\n",
        "\n",
        "    try:\n",
        "        # Image ko OpenCV se load karein\n",
        "        img = cv2.imread(source_path)\n",
        "        if img is None:\n",
        "            # print(f\"Warning: {filename} load nahi ho saki, skip kar raha hoon.\")\n",
        "            errors += 1\n",
        "            continue\n",
        "\n",
        "        # Image ki height aur width lein\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # --- Center Crop Logic ---\n",
        "        new_h = int(h * CROP_PERCENT)\n",
        "        new_w = int(w * CROP_PERCENT)\n",
        "\n",
        "        start_y = (h - new_h) // 2\n",
        "        start_x = (w - new_w) // 2\n",
        "        end_y = start_y + new_h\n",
        "        end_x = start_x + new_w\n",
        "\n",
        "        # Image ko crop karein\n",
        "        cropped_img = img[start_y:end_y, start_x:end_x]\n",
        "\n",
        "        # Nayi image ko save karein\n",
        "        cv2.imwrite(dest_path, cropped_img)\n",
        "        processed_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error file {filename}: {e}\")\n",
        "        errors += 1\n",
        "\n",
        "print(\"\\n--- CROP MUKAMMAL! ---\")\n",
        "print(f\"Total images found: {len(image_files)}\")\n",
        "print(f\"Successfully processed and saved: {processed_count}\")\n",
        "print(f\"Errors (skipped files): {errors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "2bb2ffb38ddd4fe0b1eee2e2f0231fe6",
            "3628d09ca07049beaafe1c0bfb1087d5",
            "19732df10eb8441aafc65e9cce78e62c",
            "26e221de0533475aa4ebc6bbc5466a31",
            "1610d70b061d4c71a6bd249cb1d219c9",
            "991f34b052f44c4db54924c93298a17c",
            "a6a9c37ffe2047f7b42da96cdf847810",
            "00d81685027740b28e574cdc70c228c7",
            "052d353a3d794a5f828c4779edfb2037",
            "a93264d9dd054bf2b8bfaab531d52889",
            "44bc0623853648a0b267df137804e59f"
          ]
        },
        "id": "DxUc098u7G4S",
        "outputId": "3dba2363-6282-4ab9-eaf9-127a4e9b7bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/local_images' se images ko crop kiya ja raha hai...\n",
            "Nayi images '/content/local_images_cropped' mein save hon gi...\n",
            "Cropping percentage: 75%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Cropping images:   0%|          | 0/30051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bb2ffb38ddd4fe0b1eee2e2f0231fe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- CROP MUKAMMAL! ---\n",
            "Total images found: 30051\n",
            "Successfully processed and saved: 30051\n",
            "Errors (skipped files): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "# --- Sirf Simple Augmentation ---\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. Configuration (Final Settings) ---\n",
        "BATCH_SIZE = 32 # 300x300 size ke liye\n",
        "IMG_WIDTH, IMG_HEIGHT = 300, 300 # üî¥ PURANAY MODEL SE MATCH HONA ZAROORI HAI\n",
        "EPOCHS_FINE_TUNE = 30\n",
        "LEARNING_RATE_FINE_TUNE = 1e-5 # üî¥ Bohat Low LR (Polishing ke liye)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "# üî¥ Yahan apna 79% (ya 86.35%) wala model ka path dein\n",
        "SAVED_WEIGHTS_PATH = '/content/drive/MyDrive/images/v6_okay_final_256px.keras'\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/images/v6_polished_best.keras' # Naya naam\n",
        "IMAGE_DIR = '/content/local_images_cropped' # Cropped folder\n",
        "CLEAN_CSV_PATH = '/content/drive/MyDrive/images/master_tf_clean.csv'\n",
        "\n",
        "# --- 3. (NAYA FIX) Class Weights and Names ---\n",
        "# üî¥ 'Normal' ki weight ko 0.33 se 1.0 kar diya gaya hai (taake model 'Normal' ko ignore na kare)\n",
        "CLASS_WEIGHTS_DICT = {\n",
        "    0: 1.38, # COVID-19\n",
        "    1: 0.83, # Lung Opacity\n",
        "    2: 1.0,  # üî¥ NORMAL (FIXED)\n",
        "    3: 2.23, # Pneumonia (Bacterial)\n",
        "    4: 4.0,  # Pneumonia (Viral)\n",
        "    5: 5.0   # üî¥ Tuberculosis (Thora kam kiya taake 'Normal' ko chance mile)\n",
        "}\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 4. Function Definitions ---\n",
        "\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "# (A) NAYA build_dataset function (SIRF Simple Augmentation)\n",
        "def build_dataset(df, augment=False):\n",
        "\n",
        "    # Simple Augmentation (Polishing ke liye)\n",
        "    augmentation_layers = tf.keras.Sequential([\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(0.1),\n",
        "        RandomZoom(0.1),\n",
        "    ], name=\"simple_augmentation\")\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (augmentation_layers(x, training=True), y),\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE) # .cache() hata diya gaya hai\n",
        "    return dataset\n",
        "\n",
        "# (B) NAYA build_densenet_model (Crash Fix)\n",
        "def build_densenet_model():\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model, base_model # Hum dono return karein ge\n",
        "\n",
        "# --- 5. Data Loading & Pipeline Execution ---\n",
        "print(f\"Loading data from {CLEAN_CSV_PATH}...\")\n",
        "df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "df['label_idx'] = df['label'].map(class_indices)\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "# Sirf Simple Augmentation wala dataset banayein\n",
        "train_dataset = build_dataset(train_df, augment=True)\n",
        "val_dataset = build_dataset(val_df, augment=False)\n",
        "test_dataset = build_dataset(test_df, augment=False)\n",
        "\n",
        "# --- 6. Model Build and Weight Load (Skip Stage 1) ---\n",
        "print(\"Building DenseNet-121 model structure (300x300)...\")\n",
        "model, base_model = build_densenet_model()\n",
        "\n",
        "print(f\"--- STAGE 1 SKIPPED ---\")\n",
        "print(f\"Loading best weights (RandAugment model) from '{SAVED_WEIGHTS_PATH}'...\")\n",
        "try:\n",
        "    model.load_weights(SAVED_WEIGHTS_PATH)\n",
        "    print(\"‚úÖ Weights loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Weights load nahi huin. {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 7. STAGE 3: \"Polishing\" Fine-Tuning (Simple Aug) ---\n",
        "print(\"\\n--- STAGE 3: 'Polishing' Fine-tuning (Simple Aug + New Weights) ---\")\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:347]: # Top layers ko fine-tune karein\n",
        "    layer.trainable = False\n",
        "print(\"‚úÖ DenseNet Base Model ki Top Layers Fine-tuning ke liye unfreeze kar di gayi hain.\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE), # üî¥ Bohat Low LR\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Smart Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(FINAL_MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "print(\"Starting 'Polishing' fine-tuning with Simple Augmentation...\")\n",
        "\n",
        "history_fine_tune = model.fit(\n",
        "    train_dataset, # üî¥ Simple Augmentation dataset\n",
        "    epochs=EPOCHS_FINE_TUNE,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "    class_weight=CLASS_WEIGHTS_DICT # üî¥ NAYE (V6) WEIGHTS\n",
        ")\n",
        "\n",
        "print(\"--- 'Polishing' Fine-Tuning Complete! ---\")\n",
        "\n",
        "# --- 8. Final Evaluation ---\n",
        "# (Yahan aap apna evaluation code chala saktay hain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "55jst6-G7Iav",
        "outputId": "1f1fc3eb-2615-46fb-a664-0a08d25ff847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/drive/MyDrive/images/master_tf_clean.csv...\n",
            "Building DenseNet-121 model structure (300x300)...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "--- STAGE 1 SKIPPED ---\n",
            "Loading best weights (RandAugment model) from '/content/drive/MyDrive/images/v6_okay_final_256px.keras'...\n",
            "‚úÖ Weights loaded successfully.\n",
            "\n",
            "--- STAGE 3: 'Polishing' Fine-tuning (Simple Aug + New Weights) ---\n",
            "‚úÖ DenseNet Base Model ki Top Layers Fine-tuning ke liye unfreeze kar di gayi hain.\n",
            "Starting 'Polishing' fine-tuning with Simple Augmentation...\n",
            "Epoch 1/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789ms/step - accuracy: 0.7884 - loss: 0.8111\n",
            "Epoch 1: val_loss improved from inf to 0.41889, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 852ms/step - accuracy: 0.7884 - loss: 0.8110 - val_accuracy: 0.8435 - val_loss: 0.4189 - learning_rate: 1.0000e-05\n",
            "Epoch 2/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736ms/step - accuracy: 0.8208 - loss: 0.7105\n",
            "Epoch 2: val_loss improved from 0.41889 to 0.41875, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 765ms/step - accuracy: 0.8208 - loss: 0.7105 - val_accuracy: 0.8435 - val_loss: 0.4187 - learning_rate: 1.0000e-05\n",
            "Epoch 3/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.8288 - loss: 0.6679\n",
            "Epoch 3: val_loss improved from 0.41875 to 0.40552, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 765ms/step - accuracy: 0.8288 - loss: 0.6679 - val_accuracy: 0.8459 - val_loss: 0.4055 - learning_rate: 1.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.8361 - loss: 0.6406\n",
            "Epoch 4: val_loss did not improve from 0.40552\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 767ms/step - accuracy: 0.8361 - loss: 0.6406 - val_accuracy: 0.8485 - val_loss: 0.4075 - learning_rate: 1.0000e-05\n",
            "Epoch 5/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742ms/step - accuracy: 0.8426 - loss: 0.6274\n",
            "Epoch 5: val_loss improved from 0.40552 to 0.39242, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 766ms/step - accuracy: 0.8426 - loss: 0.6274 - val_accuracy: 0.8552 - val_loss: 0.3924 - learning_rate: 1.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.8487 - loss: 0.5930\n",
            "Epoch 6: val_loss improved from 0.39242 to 0.38539, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 758ms/step - accuracy: 0.8487 - loss: 0.5929 - val_accuracy: 0.8592 - val_loss: 0.3854 - learning_rate: 1.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.8507 - loss: 0.5907\n",
            "Epoch 7: val_loss improved from 0.38539 to 0.37652, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 756ms/step - accuracy: 0.8507 - loss: 0.5907 - val_accuracy: 0.8632 - val_loss: 0.3765 - learning_rate: 1.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729ms/step - accuracy: 0.8575 - loss: 0.5693\n",
            "Epoch 8: val_loss improved from 0.37652 to 0.37173, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 751ms/step - accuracy: 0.8575 - loss: 0.5693 - val_accuracy: 0.8645 - val_loss: 0.3717 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731ms/step - accuracy: 0.8593 - loss: 0.5713\n",
            "Epoch 9: val_loss did not improve from 0.37173\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 759ms/step - accuracy: 0.8593 - loss: 0.5713 - val_accuracy: 0.8612 - val_loss: 0.3832 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726ms/step - accuracy: 0.8620 - loss: 0.5510\n",
            "Epoch 10: val_loss improved from 0.37173 to 0.36299, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 756ms/step - accuracy: 0.8620 - loss: 0.5509 - val_accuracy: 0.8688 - val_loss: 0.3630 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726ms/step - accuracy: 0.8681 - loss: 0.5347\n",
            "Epoch 11: val_loss improved from 0.36299 to 0.35416, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 756ms/step - accuracy: 0.8681 - loss: 0.5347 - val_accuracy: 0.8742 - val_loss: 0.3542 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730ms/step - accuracy: 0.8694 - loss: 0.5251\n",
            "Epoch 12: val_loss did not improve from 0.35416\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 758ms/step - accuracy: 0.8694 - loss: 0.5251 - val_accuracy: 0.8702 - val_loss: 0.3604 - learning_rate: 1.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851ms/step - accuracy: 0.8653 - loss: 0.5265\n",
            "Epoch 13: val_loss improved from 0.35416 to 0.34797, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 880ms/step - accuracy: 0.8653 - loss: 0.5265 - val_accuracy: 0.8745 - val_loss: 0.3480 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734ms/step - accuracy: 0.8716 - loss: 0.5122\n",
            "Epoch 14: val_loss did not improve from 0.34797\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m572s\u001b[0m 761ms/step - accuracy: 0.8716 - loss: 0.5122 - val_accuracy: 0.8735 - val_loss: 0.3608 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.8747 - loss: 0.4964\n",
            "Epoch 15: val_loss did not improve from 0.34797\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 767ms/step - accuracy: 0.8747 - loss: 0.4964 - val_accuracy: 0.8752 - val_loss: 0.3517 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.8774 - loss: 0.4841\n",
            "Epoch 16: val_loss improved from 0.34797 to 0.33775, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 762ms/step - accuracy: 0.8774 - loss: 0.4841 - val_accuracy: 0.8775 - val_loss: 0.3377 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738ms/step - accuracy: 0.8802 - loss: 0.4923\n",
            "Epoch 17: val_loss did not improve from 0.33775\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 759ms/step - accuracy: 0.8802 - loss: 0.4923 - val_accuracy: 0.8795 - val_loss: 0.3438 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.8837 - loss: 0.4850\n",
            "Epoch 18: val_loss did not improve from 0.33775\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m572s\u001b[0m 760ms/step - accuracy: 0.8837 - loss: 0.4850 - val_accuracy: 0.8758 - val_loss: 0.3399 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751ms/step - accuracy: 0.8826 - loss: 0.4801\n",
            "Epoch 19: val_loss did not improve from 0.33775\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 779ms/step - accuracy: 0.8826 - loss: 0.4800 - val_accuracy: 0.8775 - val_loss: 0.3397 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.8855 - loss: 0.4587\n",
            "Epoch 20: val_loss improved from 0.33775 to 0.33635, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 765ms/step - accuracy: 0.8855 - loss: 0.4587 - val_accuracy: 0.8808 - val_loss: 0.3364 - learning_rate: 2.0000e-06\n",
            "Epoch 21/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735ms/step - accuracy: 0.8877 - loss: 0.4544\n",
            "Epoch 21: val_loss improved from 0.33635 to 0.33570, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 764ms/step - accuracy: 0.8877 - loss: 0.4544 - val_accuracy: 0.8815 - val_loss: 0.3357 - learning_rate: 2.0000e-06\n",
            "Epoch 22/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733ms/step - accuracy: 0.8887 - loss: 0.4445\n",
            "Epoch 22: val_loss improved from 0.33570 to 0.33220, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 754ms/step - accuracy: 0.8887 - loss: 0.4445 - val_accuracy: 0.8802 - val_loss: 0.3322 - learning_rate: 2.0000e-06\n",
            "Epoch 23/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733ms/step - accuracy: 0.8855 - loss: 0.4538\n",
            "Epoch 23: val_loss did not improve from 0.33220\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 753ms/step - accuracy: 0.8855 - loss: 0.4538 - val_accuracy: 0.8808 - val_loss: 0.3326 - learning_rate: 2.0000e-06\n",
            "Epoch 24/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733ms/step - accuracy: 0.8869 - loss: 0.4513\n",
            "Epoch 24: val_loss improved from 0.33220 to 0.33041, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 763ms/step - accuracy: 0.8869 - loss: 0.4513 - val_accuracy: 0.8812 - val_loss: 0.3304 - learning_rate: 2.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726ms/step - accuracy: 0.8856 - loss: 0.4491\n",
            "Epoch 25: val_loss did not improve from 0.33041\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 754ms/step - accuracy: 0.8856 - loss: 0.4491 - val_accuracy: 0.8802 - val_loss: 0.3345 - learning_rate: 2.0000e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729ms/step - accuracy: 0.8912 - loss: 0.4475\n",
            "Epoch 26: val_loss improved from 0.33041 to 0.33014, saving model to /content/drive/MyDrive/images/v6_polished_best.keras\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 759ms/step - accuracy: 0.8912 - loss: 0.4475 - val_accuracy: 0.8812 - val_loss: 0.3301 - learning_rate: 2.0000e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - accuracy: 0.8864 - loss: 0.4468\n",
            "Epoch 27: val_loss did not improve from 0.33014\n",
            "\u001b[1m751/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 760ms/step - accuracy: 0.8864 - loss: 0.4468 - val_accuracy: 0.8795 - val_loss: 0.3342 - learning_rate: 2.0000e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m438/751\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m3:48\u001b[0m 729ms/step - accuracy: 0.8842 - loss: 0.4586"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1081744692.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting 'Polishing' fine-tuning with Simple Augmentation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m history_fine_tune = model.fit(\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# üî¥ Simple Augmentation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS_FINE_TUNE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9eSviNUf8qcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Configuration (Set Your Parameters) ---\n",
        "# üî¥ Yaqeen karein ke yeh aapka 88.05% wala model hai\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/images/v6_polished_best.keras'\n",
        "IMAGE_DIR = '/content/local_images_cropped'\n",
        "CLEAN_CSV_PATH = '/content/drive/MyDrive/images/master_tf_clean.csv'\n",
        "BATCH_SIZE = 32 # Evaluation ke liye 32 theek hai\n",
        "IMG_WIDTH, IMG_HEIGHT = 300, 300\n",
        "\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 1. Data Loading & Pipeline Execution ---\n",
        "print(f\"Loading final model and data...\")\n",
        "df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "df['label_idx'] = df['label'].map(class_indices)\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "_, test_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "# --- Build Dataset Function (Wohi Jo Training Mein Tha) ---\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "def build_dataset(df): # Augmentation 'False' hai\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "test_dataset = build_dataset(test_df)\n",
        "\n",
        "# --- 2. Model Load and Evaluation ---\n",
        "print(f\"Loading best weights (88.05% Peak) from '{FINAL_MODEL_PATH}'...\")\n",
        "try:\n",
        "    # Model ko uske structure ke saath load karein\n",
        "    model = tf.keras.models.load_model(FINAL_MODEL_PATH)\n",
        "    print(\"‚úÖ Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    # Agar load_model fail ho to manual tareeqa istemaal karein\n",
        "    print(f\"‚ùå Model load error: {e}. Manual build kar raha hoon...\")\n",
        "\n",
        "    # Manual build (aapka purana function)\n",
        "    def build_densenet_model():\n",
        "        base_model = DenseNet121(weights=None, include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "        base_model.trainable = True\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "        return Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    model = build_densenet_model()\n",
        "    model.load_weights(FINAL_MODEL_PATH)\n",
        "    print(\"‚úÖ Model manually loaded.\")\n",
        "\n",
        "# --- 3. Final Prediction & Report ---\n",
        "print(\"\\nGenerating classification report and confusion matrix...\")\n",
        "# Test dataset se predictions aur labels collect karein\n",
        "y_true = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for images, labels in tqdm(test_dataset, desc=\"Generating Predictions\"):\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred_probs.extend(model.predict(images, verbose=0))\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"\\n--- Final Test Set Evaluation (V6 Polished Model) ---\")\n",
        "print(f\"Total Test Samples: {len(y_true)}\")\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(\"\\n--- Classification Report (Final) ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))"
      ],
      "metadata": {
        "id": "yP8LuQ7v8dUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634,
          "referenced_widgets": [
            "61ccf844868e4fa5ba71ae7369eaac15",
            "f833527c228f4372b136c01fc7e51649",
            "9797662ea0e94dd48cb9a1d5b2751aae",
            "60fb81141abe411aa0f3ca1b32b1940c",
            "0a0c7723049e4dbabf8777f46f64201b",
            "7c3bfe514199481ea604a77f3c766f91",
            "c09875eadd8b4f82a3f572b1da704390",
            "f74a4e24e058460a9983715badc90156",
            "962a30ef824f410bb1ebb3832d91c53b",
            "52220efb599b4f22b1a7be12a79c308e",
            "7a197927e8624f8f8a455f2f7795b349"
          ]
        },
        "outputId": "6bed38b3-a2cc-4b3b-d905-03cc3bd046f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading final model and data...\n",
            "Loading best weights (88.05% Peak) from '/content/drive/MyDrive/images/v6_polished_best.keras'...\n",
            "‚úÖ Model loaded successfully.\n",
            "\n",
            "Generating classification report and confusion matrix...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Predictions:   0%|          | 0/94 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ccf844868e4fa5ba71ae7369eaac15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Test Set Evaluation (V6 Polished Model) ---\n",
            "Total Test Samples: 3004\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[ 301   10   41    0    2    8]\n",
            " [  18  435  137    0    2    9]\n",
            " [   3   11 1464    6    8    0]\n",
            " [   0    0    3  202   19    0]\n",
            " [   0    0    8   66  181    0]\n",
            " [   2    0    4    0    0   64]]\n",
            "\n",
            "--- Classification Report (Final) ---\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "             COVID-19       0.93      0.83      0.88       362\n",
            "         Lung Opacity       0.95      0.72      0.82       601\n",
            "               Normal       0.88      0.98      0.93      1492\n",
            "Pneumonia (Bacterial)       0.74      0.90      0.81       224\n",
            "    Pneumonia (Viral)       0.85      0.71      0.78       255\n",
            "         Tuberculosis       0.79      0.91      0.85        70\n",
            "\n",
            "             accuracy                           0.88      3004\n",
            "            macro avg       0.86      0.84      0.84      3004\n",
            "         weighted avg       0.89      0.88      0.88      3004\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYmAOJB1ZByj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}