{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qHo1sIojFXZ",
        "outputId": "df319c72-b502-431f-eff1-ccea01905184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setup Shuru ---\n",
            "Target Folder: /content/local_images\n",
            "Copying Copy of images2_archive.tar.gz...\n",
            "Copy complete in 9.98 seconds.\n",
            "Extracting and ADDING images to /content/local_images...\n",
            "Extraction complete in 5.33 seconds.\n",
            "\n",
            "âœ… --- READY TO TRAIN! Images are in /content/local_images ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# --- 1. Sirf is line ko change karein ---\n",
        "# ğŸ”´ Pehli baar 'images1_archive.tar.gz' likhein\n",
        "#    Doosri baar 'images2_archive.tar.gz' likhein, etc.\n",
        "ARCHIVE_FILE_NAME = \"Copy of images2_archive.tar.gz\"\n",
        "\n",
        "# --- 2. In paths ko hamesha same rehne dein ---\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/images\"\n",
        "# Yeh hamesha same folder rahega taake images jama (collect) ho sakein\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "# --- 3. Baqi code ab automatically kaam karega ---\n",
        "DRIVE_TAR_PATH = os.path.join(DRIVE_BASE_PATH, ARCHIVE_FILE_NAME)\n",
        "LOCAL_TAR_PATH = f\"/content/{ARCHIVE_FILE_NAME}\"\n",
        "\n",
        "# Yeh line check karti hai ke folder hai ya nahi. Agar hai, to usay istemaal karti hai.\n",
        "os.makedirs(LOCAL_IMAGE_DIR, exist_ok=True)\n",
        "print(f\"--- Setup Shuru ---\")\n",
        "print(f\"Target Folder: {LOCAL_IMAGE_DIR}\")\n",
        "\n",
        "# --- FAST COPY ---\n",
        "print(f\"Copying {ARCHIVE_FILE_NAME}...\")\n",
        "start_time = time.time()\n",
        "!cp \"{DRIVE_TAR_PATH}\" \"{LOCAL_TAR_PATH}\"\n",
        "print(f\"Copy complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "# --- FAST UNPACK (ADD) ---\n",
        "print(f\"Extracting and ADDING images to {LOCAL_IMAGE_DIR}...\")\n",
        "start_time = time.time()\n",
        "# Yeh command purani files ko delete nahi karta, sirf nayi files add karta hai\n",
        "!tar -xzf \"{LOCAL_TAR_PATH}\" -C \"{LOCAL_IMAGE_DIR}\"\n",
        "print(f\"Extraction complete in {(time.time() - start_time):.2f} seconds.\")\n",
        "\n",
        "print(f\"\\nâœ… --- READY TO TRAIN! Images are in {LOCAL_IMAGE_DIR} ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFI28ejfjgM8",
        "outputId": "82a6a7a3-bcb1-4b0e-b872-ed1aee511a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Success! Found 30051 images in '/content/local_images'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# The path where you extracted the images\n",
        "LOCAL_IMAGE_DIR = \"/content/local_images\"\n",
        "\n",
        "try:\n",
        "    file_count = len(os.listdir(LOCAL_IMAGE_DIR))\n",
        "    print(f\"âœ… Success! Found {file_count} images in '{LOCAL_IMAGE_DIR}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ Error: Could not find the directory '{LOCAL_IMAGE_DIR}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_glqgl8aj1wm",
        "outputId": "a5d98dd9-f8b5-444a-bd47-18a31ef26e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading clean CSV: /content/drive/MyDrive/images/master_tf_clean.csv...\n",
            "Loaded 30036 clean image records.\n",
            "Splitting data into train, validation, and test sets...\n",
            "Training images: 24028\n",
            "Validation images: 3004\n",
            "Test images: 3004\n",
            "Building tf.data pipelines...\n",
            "Building DenseNet-121 model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "--- STAGE 1: Training the new head ---\n",
            "Epoch 1/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4974 - loss: 1.2045\n",
            "Epoch 1: val_loss improved from inf to 0.72666, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 2s/step - accuracy: 0.4979 - loss: 1.2030 - val_accuracy: 0.7367 - val_loss: 0.7267\n",
            "Epoch 2/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6934 - loss: 0.6960\n",
            "Epoch 2: val_loss improved from 0.72666 to 0.65876, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 2s/step - accuracy: 0.6935 - loss: 0.6959 - val_accuracy: 0.7610 - val_loss: 0.6588\n",
            "Epoch 3/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7245 - loss: 0.6238\n",
            "Epoch 3: val_loss did not improve from 0.65876\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 2s/step - accuracy: 0.7245 - loss: 0.6238 - val_accuracy: 0.7447 - val_loss: 0.6722\n",
            "Epoch 4/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7382 - loss: 0.5850\n",
            "Epoch 4: val_loss improved from 0.65876 to 0.59800, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 2s/step - accuracy: 0.7382 - loss: 0.5850 - val_accuracy: 0.7776 - val_loss: 0.5980\n",
            "Epoch 5/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7542 - loss: 0.5614\n",
            "Epoch 5: val_loss improved from 0.59800 to 0.55675, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 2s/step - accuracy: 0.7542 - loss: 0.5613 - val_accuracy: 0.7933 - val_loss: 0.5567\n",
            "Epoch 6/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7673 - loss: 0.5457\n",
            "Epoch 6: val_loss did not improve from 0.55675\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 2s/step - accuracy: 0.7673 - loss: 0.5456 - val_accuracy: 0.7600 - val_loss: 0.6370\n",
            "Epoch 7/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7601 - loss: 0.5471\n",
            "Epoch 7: val_loss improved from 0.55675 to 0.49936, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 2s/step - accuracy: 0.7601 - loss: 0.5471 - val_accuracy: 0.8123 - val_loss: 0.4994\n",
            "Epoch 8/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7785 - loss: 0.5046\n",
            "Epoch 8: val_loss did not improve from 0.49936\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 2s/step - accuracy: 0.7785 - loss: 0.5045 - val_accuracy: 0.7989 - val_loss: 0.5445\n",
            "Epoch 9/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7815 - loss: 0.5009\n",
            "Epoch 9: val_loss improved from 0.49936 to 0.46187, saving model to /content/drive/MyDrive/images/model1.keras\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 2s/step - accuracy: 0.7815 - loss: 0.5008 - val_accuracy: 0.8342 - val_loss: 0.4619\n",
            "Epoch 10/10\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7823 - loss: 0.5095\n",
            "Epoch 10: val_loss did not improve from 0.46187\n",
            "\u001b[1m188/188\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 2s/step - accuracy: 0.7823 - loss: 0.5094 - val_accuracy: 0.7833 - val_loss: 0.5795\n",
            "Head training complete. Best weights (Stage 1) loaded.\n",
            "Best model abhi tak '/content/drive/MyDrive/images/model1.keras' mein save ho gaya hai.\n",
            "--- STAGE 2: Smart Fine-tuning the top layers ---\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'InputLayer' object has no attribute 'layers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-539896314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m# Pehli 347 layers ko frozen rakhein (yeh basic shapes seekhti hain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m347\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'InputLayer' object has no attribute 'layers'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Colab GPU ke liye BATCH_SIZE barha dein\n",
        "BATCH_SIZE = 128\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "\n",
        "EPOCHS_HEAD = 10       # Stage 1 ke liye epochs\n",
        "EPOCHS_FINE_TUNE = 20  # Stage 2 ke liye epochs\n",
        "\n",
        "LEARNING_RATE_HEAD = 1e-3\n",
        "LEARNING_RATE_FINE_TUNE = 1e-4 # Hum 1e-4 se shuru karein ge (ReduceLROnPlateau isay control karega)\n",
        "\n",
        "# --- 2. Paths ---\n",
        "# Yaqeen karein ke aapka setup cell chal chuka hai\n",
        "IMAGE_DIR = '/content/local_images'\n",
        "# Humari sab se clean CSV file\n",
        "CLEAN_CSV_PATH = '/content/drive/MyDrive/images/master_tf_clean.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/images/model1.keras'\n",
        "\n",
        "# --- 3. Class Weights (Aapke original dataset se) ---\n",
        "CLASS_WEIGHTS_DICT = {\n",
        "    0: 1.3844487463126844, 1: 0.8326957196717676, 2: 0.33555644927050515,\n",
        "    3: 2.2368930592791183, 4: 1.961664054336468, 5: 7.151666666666666\n",
        "}\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 4. Load Full (Clean) Dataset ---\n",
        "print(f\"Loading clean CSV: {CLEAN_CSV_PATH}...\")\n",
        "try:\n",
        "    df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "    # Filepath column banayein\n",
        "    df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "    # Labels ko integers (0-5) mein convert karein\n",
        "    class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "    df['label_idx'] = df['label'].map(class_indices)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV or mapping paths: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"Loaded {len(df)} clean image records.\")\n",
        "\n",
        "# --- 5. Split Data (Train, Validation, Test) ---\n",
        "print(\"Splitting data into train, validation, and test sets...\")\n",
        "# 90% train+val, 10% test\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "# 80% train, 10% val\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "print(f\"Training images: {len(train_df)}\")\n",
        "print(f\"Validation images: {len(val_df)}\")\n",
        "print(f\"Test images: {len(test_df)}\")\n",
        "\n",
        "# --- 6. tf.data Pipeline (Corrupt images ke fix ke saath) ---\n",
        "print(\"Building tf.data pipelines...\")\n",
        "\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    # decode_image har format (PNG, JPEG, etc.) ko handle karta hai\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img) # DenseNet ki preprocessing\n",
        "    return img, label\n",
        "\n",
        "def build_dataset(df, augment=False):\n",
        "    # Keras GPU-powered augmentation layers\n",
        "    augmentation_layers = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.1),\n",
        "        tf.keras.layers.RandomZoom(0.1),\n",
        "    ])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    # AUTOTUNE Colab par behtar kaam karega (RAM ka masla nahi hoga)\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (augmentation_layers(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "     # Memory mein cache karein (Colab par fast hai)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = build_dataset(train_df, augment=True)\n",
        "val_dataset = build_dataset(val_df)\n",
        "test_dataset = build_dataset(test_df) # Yeh aakhir mein istemaal hoga\n",
        "\n",
        "# --- 7. Build Model Function ---\n",
        "def build_densenet_model():\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    base_model.trainable = False # Shuru mein base model ko freeze rakhein\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "print(\"Building DenseNet-121 model...\")\n",
        "model = build_densenet_model()\n",
        "\n",
        "# --- 8. STAGE 1: Train the Head (Poora Code) ---\n",
        "print(\"--- STAGE 1: Training the new head ---\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_HEAD),\n",
        "    loss='sparse_categorical_crossentropy', # Kyunke labels integers (0-5) hain\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks (Stage 1 ke liye)\n",
        "early_stopping_s1 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint_s1 = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "history_head = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS_HEAD,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping_s1, model_checkpoint_s1],\n",
        "    class_weight=CLASS_WEIGHTS_DICT # Class weights zaroor istemaal karein\n",
        ")\n",
        "\n",
        "print(\"Head training complete. Best weights (Stage 1) loaded.\")\n",
        "print(f\"Best model abhi tak '{MODEL_SAVE_PATH}' mein save ho gaya hai.\")\n",
        "\n",
        "# --- 9. STAGE 2: Smart Fine-Tuning ---\n",
        "print(\"--- STAGE 2: Smart Fine-tuning the top layers ---\")\n",
        "\n",
        "# (A) Sirf top layers ko unfreeze karein\n",
        "base_model = model.layers[0] # DenseNet base model ko select karein\n",
        "base_model.trainable = True\n",
        "\n",
        "# Pehli 347 layers ko frozen rakhein (yeh basic shapes seekhti hain)\n",
        "for layer in base_model.layers[:347]:\n",
        "    layer.trainable = False\n",
        "\n",
        "print(f\"Total layers in base_model: {len(base_model.layers)}\")\n",
        "print(\"First 347 layers frozen. Only top layers will be fine-tuned.\")\n",
        "\n",
        "# (B) Model ko naye LR se compile karein\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# (C) Naye (Smart) Callbacks banayein\n",
        "# Patience ko 10 kar dein\n",
        "early_stopping_s2 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "# Nayi file save karein (taake Stage 1 ka model mehfooz rahe)\n",
        "model_checkpoint_s2 = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "# Yeh naya callback hai\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "print(\"Starting smart fine-tuning...\")\n",
        "\n",
        "# (D) Training run karein\n",
        "history_fine_tune = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS_FINE_TUNE,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping_s2, model_checkpoint_s2, reduce_lr], # Nayi list\n",
        "    class_weight=CLASS_WEIGHTS_DICT\n",
        ")\n",
        "\n",
        "print(\"--- Smart Fine-Tuning Complete! ---\")\n",
        "print(f\"Final best model '{MODEL_SAVE_PATH}' mein save ho gaya hai.\")\n",
        "\n",
        "# --- 10. Final Evaluation ---\n",
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "print(f\"Loading best model from '{MODEL_SAVE_PATH}'...\")\n",
        "# ModelCheckpoint ne jo sab se best model save kiya tha (chahe Stage 1 ya 2 se), usay load karein\n",
        "model.load_weights(MODEL_SAVE_PATH)\n",
        "\n",
        "print(\"Evaluating model on the hold-out test set...\")\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "# --- IMPORTANT: Generate Classification Report ---\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\nGenerating classification report and confusion matrix...\")\n",
        "# Test dataset se predictions aur labels collect karein\n",
        "y_true = []\n",
        "y_pred_probs = []\n",
        "\n",
        "# tqdm progress bar\n",
        "for images, labels in tqdm(test_dataset, desc=\"Generating Predictions\"):\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred_probs.extend(model.predict(images, verbose=0))\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(\"\\n--- Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r4qBKClRqjb",
        "outputId": "bc747609-e95c-4b5b-a71e-cfcfe719619e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from /content/drive/MyDrive/images/master_tf_clean.csv...\n",
            "Building DenseNet-121 model structure...\n",
            "--- STAGE 1 SKIPPED ---\n",
            "Loading best weights (83.42% Peak) from '/content/drive/MyDrive/images/model1.keras'...\n",
            "âœ… Weights loaded successfully. Starting Fine-Tuning from 83.42%.\n",
            "\n",
            "--- STAGE 2: Simple Fine-tuning the FULL model ---\n",
            "âœ… Poora Base Model (Index 4) Fine-tuning ke liye unfreeze kar diya gaya hai.\n",
            "Epoch 1/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7945 - loss: 0.4745   \n",
            "Epoch 1: val_loss improved from inf to 0.48736, saving model to /content/drive/MyDrive/images/v2_finetuned_best.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 1s/step - accuracy: 0.7945 - loss: 0.4745 - val_accuracy: 0.8202 - val_loss: 0.4874 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980ms/step - accuracy: 0.7956 - loss: 0.4722\n",
            "Epoch 2: val_loss did not improve from 0.48736\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 1s/step - accuracy: 0.7956 - loss: 0.4722 - val_accuracy: 0.8152 - val_loss: 0.4901 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973ms/step - accuracy: 0.7977 - loss: 0.4595\n",
            "Epoch 3: val_loss improved from 0.48736 to 0.48476, saving model to /content/drive/MyDrive/images/v2_finetuned_best.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 1s/step - accuracy: 0.7977 - loss: 0.4595 - val_accuracy: 0.8179 - val_loss: 0.4848 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.8041 - loss: 0.4465\n",
            "Epoch 4: val_loss improved from 0.48476 to 0.48378, saving model to /content/drive/MyDrive/images/v2_finetuned_best.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 1s/step - accuracy: 0.8041 - loss: 0.4465 - val_accuracy: 0.8169 - val_loss: 0.4838 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976ms/step - accuracy: 0.8081 - loss: 0.4495\n",
            "Epoch 5: val_loss improved from 0.48378 to 0.48116, saving model to /content/drive/MyDrive/images/v2_finetuned_best.keras\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 1s/step - accuracy: 0.8081 - loss: 0.4495 - val_accuracy: 0.8189 - val_loss: 0.4812 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - accuracy: 0.8055 - loss: 0.4596\n",
            "Epoch 6: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 1s/step - accuracy: 0.8055 - loss: 0.4596 - val_accuracy: 0.8166 - val_loss: 0.4891 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962ms/step - accuracy: 0.8043 - loss: 0.4516\n",
            "Epoch 7: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 999ms/step - accuracy: 0.8043 - loss: 0.4516 - val_accuracy: 0.8139 - val_loss: 0.4937 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958ms/step - accuracy: 0.8071 - loss: 0.4422\n",
            "Epoch 8: val_loss did not improve from 0.48116\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 1s/step - accuracy: 0.8071 - loss: 0.4422 - val_accuracy: 0.8152 - val_loss: 0.4943 - learning_rate: 1.0000e-05\n",
            "Epoch 9/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972ms/step - accuracy: 0.8093 - loss: 0.4365\n",
            "Epoch 9: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 1s/step - accuracy: 0.8093 - loss: 0.4365 - val_accuracy: 0.8152 - val_loss: 0.4944 - learning_rate: 2.0000e-06\n",
            "Epoch 10/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.8070 - loss: 0.4545\n",
            "Epoch 10: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 1s/step - accuracy: 0.8070 - loss: 0.4545 - val_accuracy: 0.8149 - val_loss: 0.4942 - learning_rate: 2.0000e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965ms/step - accuracy: 0.8086 - loss: 0.4307\n",
            "Epoch 11: val_loss did not improve from 0.48116\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 1s/step - accuracy: 0.8086 - loss: 0.4307 - val_accuracy: 0.8172 - val_loss: 0.4916 - learning_rate: 2.0000e-06\n",
            "Epoch 12/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966ms/step - accuracy: 0.8083 - loss: 0.4419\n",
            "Epoch 12: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 1s/step - accuracy: 0.8083 - loss: 0.4419 - val_accuracy: 0.8169 - val_loss: 0.4918 - learning_rate: 4.0000e-07\n",
            "Epoch 13/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964ms/step - accuracy: 0.8137 - loss: 0.4483\n",
            "Epoch 13: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 1s/step - accuracy: 0.8137 - loss: 0.4482 - val_accuracy: 0.8176 - val_loss: 0.4916 - learning_rate: 4.0000e-07\n",
            "Epoch 14/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.8126 - loss: 0.4353\n",
            "Epoch 14: val_loss did not improve from 0.48116\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 1s/step - accuracy: 0.8126 - loss: 0.4353 - val_accuracy: 0.8172 - val_loss: 0.4921 - learning_rate: 4.0000e-07\n",
            "Epoch 15/20\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968ms/step - accuracy: 0.8125 - loss: 0.4367\n",
            "Epoch 15: val_loss did not improve from 0.48116\n",
            "\u001b[1m376/376\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 1s/step - accuracy: 0.8125 - loss: 0.4367 - val_accuracy: 0.8176 - val_loss: 0.4921 - learning_rate: 1.0000e-07\n",
            "Epoch 15: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "--- Simple Fine-Tuning Complete! ---\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. Configuration (Set Your Parameters) ---\n",
        "BATCH_SIZE = 64\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "\n",
        "EPOCHS_FINE_TUNE = 20\n",
        "# Hum purana (original) low learning rate istemaal karein ge\n",
        "LEARNING_RATE_FINE_TUNE = 1e-5\n",
        "\n",
        "# --- 2. Paths (Apne hisaab se set karein) ---\n",
        "SAVED_WEIGHTS_PATH = '/content/drive/MyDrive/images/model1.keras'\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/images/v2_finetuned_best.keras'\n",
        "IMAGE_DIR = '/content/local_images'\n",
        "CLEAN_CSV_PATH = '/content/drive/MyDrive/images/master_tf_clean.csv'\n",
        "\n",
        "# --- 3. Class Weights and Names ---\n",
        "CLASS_WEIGHTS_DICT = {\n",
        "    0: 1.38, 1: 0.83, 2: 0.33, 3: 2.23, 4: 1.96, 5: 7.15\n",
        "}\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 4. Function Definitions (Simplified) ---\n",
        "\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "def build_dataset(df, augment=False):\n",
        "    augmentation_layers = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(0.1),\n",
        "        tf.keras.layers.RandomZoom(0.1),\n",
        "    ])\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (augmentation_layers(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "def build_densenet_model():\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    base_model.trainable = False\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    return model\n",
        "\n",
        "# --- 5. Data Loading & Pipeline Execution ---\n",
        "print(f\"Loading data from {CLEAN_CSV_PATH}...\")\n",
        "df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "df['label_idx'] = df['label'].map(class_indices)\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "train_dataset = build_dataset(train_df, augment=True)\n",
        "val_dataset = build_dataset(val_df)\n",
        "test_dataset = build_dataset(test_df)\n",
        "\n",
        "# --- 6. Model Build and Weight Load (Skip Stage 1) ---\n",
        "print(\"Building DenseNet-121 model structure...\")\n",
        "model = build_densenet_model()\n",
        "\n",
        "print(f\"--- STAGE 1 SKIPPED ---\")\n",
        "print(f\"Loading best weights (83.42% Peak) from '{SAVED_WEIGHTS_PATH}'...\")\n",
        "try:\n",
        "    model.load_weights(SAVED_WEIGHTS_PATH)\n",
        "    print(\"âœ… Weights loaded successfully. Starting Fine-Tuning from 83.42%.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERROR: Weights load nahi huin. {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 7. STAGE 2: Simple Fine-Tuning Execution (PURANA CODE) ---\n",
        "print(\"\\n--- STAGE 2: Simple Fine-tuning the FULL model ---\")\n",
        "\n",
        "# (A) Index 4 par Base model ko pakrein\n",
        "base_model = model.layers[4]\n",
        "\n",
        "# (B) Poora Base Model unfreeze karein\n",
        "base_model.trainable = True\n",
        "\n",
        "print(\"âœ… Poora Base Model (Index 4) Fine-tuning ke liye unfreeze kar diya gaya hai.\")\n",
        "\n",
        "# (C) Model ko compile karein\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# (D) Callbacks (Puranay callbacks)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint(FINAL_MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# NAYA CALLBACK: Agar 3 epochs tak behtari na ho to LR kam karo\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "\n",
        "# (E) Training run karein\n",
        "history_fine_tune = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS_FINE_TUNE,\n",
        "    validation_data=val_dataset,\n",
        "    # Yahan list ko update karein\n",
        "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
        "    class_weight=CLASS_WEIGHTS_DICT\n",
        ")\n",
        "print(\"--- Simple Fine-Tuning Complete! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614,
          "referenced_widgets": [
            "c64af736c16b4c0c9a6d1233580102bf",
            "f5268d31075248eeae44fe3906ef08da",
            "4af29a7ec8c041689f305e76d2134acf",
            "3f8db7c3109a42798c305bba0b43f78f",
            "f73a102c75b84a54960b5a50cd345f8e",
            "36860440f78d4e57a97e1a3613b4e977",
            "192d0bd894504b7c8ef056ba78bea22e",
            "02a1e6d528264182a01b4d8a6dfa6b59",
            "d41a57e68c2748689fa5b55dc40e91ce",
            "52414b90590943b39d18ee9f50dd5e7e",
            "9e1bdfa30a7941cd9ebadd8417fc70ed"
          ]
        },
        "id": "sqUL6yZk81LN",
        "outputId": "7ac1ed28-745f-4d72-fdd2-92cfd17f674b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading final model and data...\n",
            "Loading best weights from '/content/drive/MyDrive/images/v2_finetuned_best.keras'...\n",
            "âœ… Model loaded successfully.\n",
            "\n",
            "Generating classification report and confusion matrix...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c64af736c16b4c0c9a6d1233580102bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Predictions:   0%|          | 0/47 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final Test Set Evaluation ---\n",
            "Total Test Samples: 3004\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[ 284   22   43    1    1   11]\n",
            " [  42  466   84    2    0    7]\n",
            " [  59   58 1308   41    5   21]\n",
            " [   0    0    1  215    8    0]\n",
            " [   0    0    8  126  121    0]\n",
            " [   2    0    2    0    0   66]]\n",
            "\n",
            "--- Classification Report (V2 Fine-Tuned) ---\n",
            "                       precision    recall  f1-score   support\n",
            "\n",
            "             COVID-19       0.73      0.78      0.76       362\n",
            "         Lung Opacity       0.85      0.78      0.81       601\n",
            "               Normal       0.90      0.88      0.89      1492\n",
            "Pneumonia (Bacterial)       0.56      0.96      0.71       224\n",
            "    Pneumonia (Viral)       0.90      0.47      0.62       255\n",
            "         Tuberculosis       0.63      0.94      0.75        70\n",
            "\n",
            "             accuracy                           0.82      3004\n",
            "            macro avg       0.76      0.80      0.76      3004\n",
            "         weighted avg       0.84      0.82      0.82      3004\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Zaroori imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Configuration (Set Your Parameters) ---\n",
        "# Yahan woh file honi chahiye jismein 81.89% ki best weights hain\n",
        "FINAL_MODEL_PATH = '/content/drive/MyDrive/images/v2_finetuned_best.keras'\n",
        "IMAGE_DIR = '/content/local_images'\n",
        "CLEAN_CSV_PATH = '/content/drive/MyDrive/images/master_tf_clean.csv'\n",
        "BATCH_SIZE = 64\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "\n",
        "CLASS_NAMES = ['COVID-19', 'Lung Opacity', 'Normal', 'Pneumonia (Bacterial)', 'Pneumonia (Viral)', 'Tuberculosis']\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# --- 1. Data Loading & Pipeline Execution (Must match training setup) ---\n",
        "print(f\"Loading final model and data...\")\n",
        "df = pd.read_csv(CLEAN_CSV_PATH)\n",
        "df['filepath'] = df['filename'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "class_indices = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "df['label_idx'] = df['label'].map(class_indices)\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
        "# Test dataset Final Evaluation ke liye\n",
        "_, test_df = train_test_split(train_val_df, test_size=(len(test_df)/len(train_val_df)), random_state=42, stratify=train_val_df['label'])\n",
        "\n",
        "# --- Build Dataset Function (Wohi Jo Training Mein Tha) ---\n",
        "def load_and_preprocess(filepath, label):\n",
        "    img = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    return img, label\n",
        "\n",
        "def build_dataset(df):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filepath'].values, df['label_idx'].values))\n",
        "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "test_dataset = build_dataset(test_df)\n",
        "\n",
        "# --- 2. Model Load and Evaluation ---\n",
        "print(f\"Loading best weights from '{FINAL_MODEL_PATH}'...\")\n",
        "try:\n",
        "    # Model ko uske structure ke saath load karein\n",
        "    model = tf.keras.models.load_model(FINAL_MODEL_PATH)\n",
        "    print(\"âœ… Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    # Agar load_model fail ho to manual tareeqa istemaal karein\n",
        "    print(\"âŒ Model load error. Manual build and weight load ki zaroorat hai.\")\n",
        "\n",
        "    # Manual build (aapka purana function)\n",
        "    def build_densenet_model():\n",
        "        base_model = DenseNet121(weights=None, include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "        base_model.trainable = True # Fine-tuned model load kar rahe hain, isliye trainable=True rehne dein\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "        return Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    model = build_densenet_model()\n",
        "    model.load_weights(FINAL_MODEL_PATH)\n",
        "\n",
        "# --- 3. Final Prediction & Report ---\n",
        "print(\"\\nGenerating classification report and confusion matrix...\")\n",
        "# Test dataset se predictions aur labels collect karein\n",
        "y_true = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for images, labels in tqdm(test_dataset, desc=\"Generating Predictions\"):\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred_probs.extend(model.predict(images, verbose=0))\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"\\n--- Final Test Set Evaluation ---\")\n",
        "print(f\"Total Test Samples: {len(y_true)}\")\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(\"\\n--- Classification Report (V2 Fine-Tuned) ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZPgWiDtA78TL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02a1e6d528264182a01b4d8a6dfa6b59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192d0bd894504b7c8ef056ba78bea22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36860440f78d4e57a97e1a3613b4e977": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8db7c3109a42798c305bba0b43f78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52414b90590943b39d18ee9f50dd5e7e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9e1bdfa30a7941cd9ebadd8417fc70ed",
            "value": "â€‡47/47â€‡[00:52&lt;00:00,â€‡â€‡4.62s/it]"
          }
        },
        "4af29a7ec8c041689f305e76d2134acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02a1e6d528264182a01b4d8a6dfa6b59",
            "max": 47,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d41a57e68c2748689fa5b55dc40e91ce",
            "value": 47
          }
        },
        "52414b90590943b39d18ee9f50dd5e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e1bdfa30a7941cd9ebadd8417fc70ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c64af736c16b4c0c9a6d1233580102bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5268d31075248eeae44fe3906ef08da",
              "IPY_MODEL_4af29a7ec8c041689f305e76d2134acf",
              "IPY_MODEL_3f8db7c3109a42798c305bba0b43f78f"
            ],
            "layout": "IPY_MODEL_f73a102c75b84a54960b5a50cd345f8e"
          }
        },
        "d41a57e68c2748689fa5b55dc40e91ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5268d31075248eeae44fe3906ef08da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36860440f78d4e57a97e1a3613b4e977",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_192d0bd894504b7c8ef056ba78bea22e",
            "value": "Generatingâ€‡Predictions:â€‡100%"
          }
        },
        "f73a102c75b84a54960b5a50cd345f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}